{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cb8508-1491-4437-b920-febefe9aeb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38a931-a5c4-4e02-be16-4776e9baba95",
   "metadata": {},
   "source": [
    "### 1. Using different inputs and outputs components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd220708-b3b4-4101-9953-f75d8e86ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name):\n",
    "    return \"Hello, \" + name + \"!\"\n",
    "\n",
    "# text, image, audio, label, gr.Textbox(lines=2, placeholder=\"Name here\")\n",
    "# All the widgets you can find here https://www.gradio.app/docs/components\n",
    "\n",
    "# iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "iface = gr.Interface(fn=greet, inputs=gr.Textbox(lines=2, placeholder=\"Name here\"), outputs=\"text\")\n",
    "iface.launch() # Use iface.close() to close the port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e40482-f736-476d-b94a-829e64d5db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close() # To close the port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4642b-9851-4d92-8cf9-855445ce2f03",
   "metadata": {},
   "source": [
    "### 2. Using different inputs, outputs and slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98670c0-99a7-4dbd-94d9-76c565c834ab",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def greet(name, is_morning, temperature):\n",
    "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
    "    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
    "    celsius = (temperature - 32) * 5/9\n",
    "    return greeting, round(celsius,2)\n",
    "# text, image, audic label\n",
    "demo = gr.Interface(fn=greet, \n",
    "                    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n",
    "                    outputs=[\"text\", \"number\"])\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e31ca6-b27a-45ae-b611-101888ca4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close() # gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8acb9c-d26c-4c8d-bf91-87de523bc613",
   "metadata": {},
   "source": [
    "### 3. Using Image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d0f8e-ed7f-4a02-afd6-4be6454f87e2",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769,0.189],\n",
    "        [0.349, 0.686, 0.168],\n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    print(input_img.shape, sepia_img.shape)\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, \n",
    "                    gr.Image(height=200,width= 200),\n",
    "                    \"image\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa25dbc-74fa-4ae5-b565-1624a13068c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close() # gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59989257-c7a1-4a64-91cb-8264366d238c",
   "metadata": {},
   "source": [
    "### 4. Using Audio as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155ee2d-ff57-4d54-92ce-1f44705cf80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(name):\n",
    "    # speech to text here...\n",
    "    return \"This is the text\"\n",
    "\n",
    "demo = gr.Interface(fn=speech_to_text,\n",
    "                    inputs=gr.Audio(label=\"Audio file\"),\n",
    "                    outputs=gr.Text())\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b1a3e-5ad6-4b70-83f3-8744ca11ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close() # gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec5822-09bb-450b-b0ec-98c713b9d9f5",
   "metadata": {},
   "source": [
    "### 5. Using Gradio Blocks (Example 1) IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a25e1-b7d0-4a29-899f-34379e15e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet (name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "    greet_btn.click(fn=greet,inputs=name, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7940b-8773-4f44-bb2b-2d70eec07394",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close() # gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61933c-2ca9-4c45-817e-d679c9c40deb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 6. Using Gradio Blocks with Examples IMP\n",
    "If you already have a code in git repository in github then you can directly import it into huggingface using below commands\n",
    "<br> git remote add space https://huggingface.co/spaces/FULL_SPACE_NAME\n",
    "<br> git push --force space main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bf761-a86f-48fb-aa40-96b8c7803481",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# pipe = pipeline(\"translation\", model=\"t5-base\")\n",
    "pipe = pipeline(\"translation_en_to_fr\", model=\"t5-base\")\n",
    "\n",
    "def translate(text):\n",
    "    return pipe(text)[0][\"translation_text\"]\n",
    "\n",
    "# Layout\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            english = gr.Textbox(label=\"English text\")\n",
    "            translate_btn = gr.Button(\"Translate\")\n",
    "        with gr.Column():\n",
    "            french = gr.Textbox(label=\"French Text\")\n",
    "            # german = gr.Textbox(label=\"German Text\")\n",
    "\n",
    "    translate_btn.click(translate, inputs=english, outputs=french)\n",
    "    examples= gr.Examples(examples=[\"I went to the supermarket yesterday\",\"Helen is a good swimmer.\"],inputs=english) # inputs=[english]\n",
    "\n",
    "# demo.launch(share=True, degug=True) # set share=True for sharing Gradio app using public url\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd448528-c583-4f41-9368-f727e9f195dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384847b-7db8-411e-8d52-8eee88c4169a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 7. Global State vs Session State (we are using in below code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24033b14-cd62-4598-ae7d-ca4890b64c10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    message = message.lower()\n",
    "    if message.startswith(\"how many\"):\n",
    "        response = random.randint(1,10)\n",
    "    elif message.startswith(\"how\"):\n",
    "        response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"])\n",
    "    elif message.startswith(\"where\"):\n",
    "        response = random.choice([\"Here\", \"There\", \"Somewhere\"])\n",
    "    else:\n",
    "        response = \"I don't know\"\n",
    "    history.append((message, response))\n",
    "    return history, history\n",
    "\n",
    "chatbot = gr.Chatbot(height=500)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    chat,\n",
    "    [\"text\", \"state\"],\n",
    "    [chatbot, \"state\"],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7b0a1-8217-47ed-8d01-d7c4301c7b82",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "demo.close() # gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c0aee-22dc-4c89-bf10-4d7771f6ba51",
   "metadata": {},
   "source": [
    "### Text Summarization with distilbart-cnn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e13cef99-54d3-4168-b608-b41489038318",
   "metadata": {},
   "source": [
    "Small specialist model\n",
    "• Designed for a specific task\n",
    "• Similar performance as a general purpose LLM\n",
    "• Cheaper and faster to run compared to a general purpose LLM\n",
    "BART large CNN Model\n",
    "• We are using the 'shleifer/distilbart-cnn-12-6', a 306M parameter distilled model trained by Facebook.\n",
    "• This model uses a process called distillation.\n",
    "• Distillation uses the predictions of a large model to train a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b27d6e-3df3-44a1-8ab0-1c679dcc54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "get_completion = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "# demo = gr.Interface(fn=summarize, inputs=\"text\", outputs=\"text\")\n",
    "demo = gr.Interface(fn=summarize, inputs=[gr.Textbox(label=\"Text to Summarize\", lines=6)], outputs=[gr.Textbox(label=\"Result\", lines=3)], title=\"Text summarization with distilbart-cnn\", description=\"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!\")\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1'])) # To open on which ever port you want\n",
    "# demo.launch(share=True)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f926f0e-0f65-4ef9-866f-71437cadc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ab5a2-5771-45ac-98df-0e487efdf58b",
   "metadata": {},
   "source": [
    "If you want to use summarization end point below is the snippet (then refer L1_NLP_tasks_with_a_simple_interface.ipynb under E:\\ANG\\Gradio\\Building Generative AI Applications with Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01853836-39fc-4e6d-8491-349f9c1c0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']\n",
    "\n",
    "# Helper function\n",
    "import requests, json\n",
    "\n",
    "#Summarization endpoint present at the server\n",
    "def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']): \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL, headers=headers,\n",
    "                                data=json.dumps(data)\n",
    "                               )\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97191cbc-5060-4b16-98b8-dce45aa981df",
   "metadata": {},
   "source": [
    "### Building a Named Entity Recognition app\n",
    "In case if you want to run ner end point (Text to Text)\n",
    "```py\n",
    "API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
    "text = \"My name is Andrew, I'm building DeepLearningAI and I live in California\"\n",
    "get_completion(text, parameters=None, ENDPOINT_URL= API_URL)\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77c21b9-afb3-4445-aa69-df533b5f19a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "\n",
    "gr.close_all() # Closes all the opened ports\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    #Here we introduce a new tag, examples, easy to use examples for your application\n",
    "                    examples=[\"My name is Andrew and I live in California\", \"My name is Poli and work at HuggingFace\"])\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873cb62-2014-4e42-9378-ab74836e627d",
   "metadata": {},
   "source": [
    "##### Adding a helper function to merge tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb06c793-fc34-4a48-84dd-2382751aae6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            # If current token continues the entity of the last one, merge them\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            # Otherwise, add the token to the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    return merged_tokens\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    # output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Poli, I live in Vienna and work at HuggingFace\"])\n",
    "\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT4']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26febd0",
   "metadata": {},
   "source": [
    "### Image Captioning app\n",
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `Salesforce/blip-image-captioning-base` a 14M parameter captioning model.\n",
    "<br> Refer Image captioning end point L2_Image_captioning_app\n",
    "<br> The code would look very similar if you were running an API (Image to Text end point) instead of it from locally. You can check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63ab54-b928-4b51-969d-093285dc3f20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "\n",
    "get_completion = pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "def image_to_base64_str(pil_image):\n",
    "    byte_arr = io.BytesIO()\n",
    "    pil_image.save(byte_arr, format='PNG')\n",
    "    byte_arr = byte_arr.getvalue()\n",
    "    return str(base64.b64encode(byte_arr).decode('utf-8'))\n",
    "\n",
    "def captioner(image):\n",
    "    base64_image = image_to_base64_str(image)\n",
    "    result = get_completion(base64_image)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=captioner,\n",
    "                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\n",
    "                    outputs=[gr.Textbox(label=\"Caption\")],\n",
    "                    title=\"Image Captioning with BLIP\",\n",
    "                    description=\"Caption any image using the BLIP model\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"christmas_dog.jpeg\", \"bird_flight.jpeg\", \"cow.jpeg\"])\n",
    "\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "display(IPython.display.Image(url=image_url))\n",
    "get_completion(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0bdb1",
   "metadata": {},
   "source": [
    "### Image Generation app\n",
    "For Text to Image generation end point api refer L3_Image_generation_app\n",
    "<br> Here we are going to run `runwayml/stable-diffusion-v1-5` using the `diffusers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import base64 \n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    return pipeline(prompt).images[0]\n",
    "\n",
    "prompt = \"a dog in a park\"\n",
    "\n",
    "result = get_completion(prompt)\n",
    "\n",
    "IPython.display.HTML(f'<img src=\"data:image/png;base64,{result}\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce944b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A helper function to convert the PIL image to base64 so you can send it to the API\n",
    "def base64_to_pil(img_base64):\n",
    "    base64_decoded = base64.b64decode(img_base64)\n",
    "    byte_stream = io.BytesIO(base64_decoded)\n",
    "    pil_image = Image.open(byte_stream)\n",
    "    return pil_image\n",
    "\n",
    "def generate(prompt):\n",
    "    output = get_completion(prompt)\n",
    "    result_image = base64_to_pil(output)\n",
    "    return result_image\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=generate,\n",
    "                    inputs=[gr.Textbox(label=\"Your prompt\")],\n",
    "                    outputs=[gr.Image(label=\"Result\")],\n",
    "                    title=\"Image Generation with Stable Diffusion\",\n",
    "                    description=\"Generate any image with Stable Diffusion\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"the spirit of a tamagotchi wandering in the city of Vienna\",\"a mecha robot in a favela\"])\n",
    "\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4071383",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a7330",
   "metadata": {},
   "source": [
    "### Building a more Advanced Interface (Blocks are preferred. Go directly to Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c844370-4c46-4115-858e-f5cd5fe337ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "\n",
    "#A helper function to convert the PIL image to base64 so you can send it to the API\n",
    "def base64_to_pil(img_base64):\n",
    "    base64_decoded = base64.b64decode(img_base64)\n",
    "    byte_stream = io.BytesIO(base64_decoded)\n",
    "    pil_image = Image.open(byte_stream)\n",
    "    return pil_image\n",
    "\n",
    "def generate(prompt, negative_prompt, steps, guidance, width, height):\n",
    "    params = {\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"num_inference_steps\": steps,\n",
    "        \"guidance_scale\": guidance,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "    \n",
    "    output = get_completion(prompt)\n",
    "    pil_image = base64_to_pil(output)\n",
    "    return pil_image\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=generate,\n",
    "                    inputs=[\n",
    "                        gr.Textbox(label=\"Your prompt\"),\n",
    "                        gr.Textbox(label=\"Negative prompt\"),\n",
    "                        gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=25,\n",
    "                                 info=\"In how many steps will the denoiser denoise the image?\"),\n",
    "                        gr.Slider(label=\"Guidance Scale\", minimum=1, maximum=20, value=7, \n",
    "                                  info=\"Controls how much the text prompt influences the result\"),\n",
    "                        gr.Slider(label=\"Width\", minimum=64, maximum=512, step=64, value=512),\n",
    "                        gr.Slider(label=\"Height\", minimum=64, maximum=512, step=64, value=512),\n",
    "                    ],\n",
    "                    outputs=[gr.Image(label=\"Result\")],\n",
    "                    title=\"Image Generation with Stable Diffusion\",\n",
    "                    description=\"Generate any image with Stable Diffusion\",\n",
    "                    allow_flagging=\"never\"\n",
    "                    )\n",
    "\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71999364",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95b525",
   "metadata": {},
   "source": [
    "### Blocks `gr.Blocks()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6818b5e-1342-439b-b129-a8904719d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Image Generation with Stable Diffusion\")\n",
    "    prompt = gr.Textbox(label=\"Your prompt\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            negative_prompt = gr.Textbox(label=\"Negative prompt\")\n",
    "            steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=25,\n",
    "                      info=\"In many steps will the denoiser denoise the image?\")\n",
    "            guidance = gr.Slider(label=\"Guidance Scale\", minimum=1, maximum=20, value=7,\n",
    "                      info=\"Controls how much the text prompt influences the result\")\n",
    "            width = gr.Slider(label=\"Width\", minimum=64, maximum=512, step=64, value=512)\n",
    "            height = gr.Slider(label=\"Height\", minimum=64, maximum=512, step=64, value=512)\n",
    "            btn = gr.Button(\"Submit\")\n",
    "        with gr.Column():\n",
    "            output = gr.Image(label=\"Result\")\n",
    "\n",
    "    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])\n",
    "gr.close_all()\n",
    "\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ef14d",
   "metadata": {},
   "source": [
    "##### More Simplistic UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655908ab-fa08-4b86-b218-f685f62d3638",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m                     height \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mSlider(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m, minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, maximum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     19\u001b[0m     output \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mImage(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#Move the output up too\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     btn\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate\u001b[49m, inputs\u001b[38;5;241m=\u001b[39m[prompt,negative_prompt,steps,guidance,width,height], outputs\u001b[38;5;241m=\u001b[39m[output])\n\u001b[0;32m     23\u001b[0m gr\u001b[38;5;241m.\u001b[39mclose_all()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# demo.launch(share=True, server_port=int(os.environ['PORT4']))\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Image Generation with Stable Diffusion\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            prompt = gr.Textbox(label=\"Your prompt\") #Give prompt some real estate\n",
    "        with gr.Column(scale=1, min_width=50):\n",
    "            btn = gr.Button(\"Submit\") #Submit button side by side!\n",
    "    with gr.Accordion(\"Advanced options\", open=False): #Let's hide the advanced options!\n",
    "            negative_prompt = gr.Textbox(label=\"Negative prompt\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=25,\n",
    "                      info=\"In many steps will the denoiser denoise the image?\")\n",
    "                    guidance = gr.Slider(label=\"Guidance Scale\", minimum=1, maximum=20, value=7,\n",
    "                      info=\"Controls how much the text prompt influences the result\")\n",
    "                with gr.Column():\n",
    "                    width = gr.Slider(label=\"Width\", minimum=64, maximum=512, step=64, value=512)\n",
    "                    height = gr.Slider(label=\"Height\", minimum=64, maximum=512, step=64, value=512)\n",
    "    output = gr.Image(label=\"Result\") #Move the output up too\n",
    "            \n",
    "    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])\n",
    "\n",
    "gr.close_all()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT4']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d91ece7c-f5a5-4025-8558-23bd2e007ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fce4f",
   "metadata": {},
   "source": [
    "<br> E\\ANG\\Gradio\\Building Generative AI Applications with Gradio\\4 Describe and Generate Game.mp4\n",
    "<br> E:\\ANG\\Gradio\\Building Generative AI Applications with Gradio\\5 Chat with any LLM.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "450px",
    "left": "1203px",
    "top": "110.525px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

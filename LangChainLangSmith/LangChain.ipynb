{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d872915",
   "metadata": {},
   "source": [
    "# LangChain using ChatGPT LLM (using LLM Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5583ebe1-db8f-451c-9a55-fcc1e7661569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f8ff0",
   "metadata": {},
   "source": [
    "## <span style=\"color:Brown\">LangChain Basics</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a97857ee-f3bd-4fcf-b8b0-52f1ccd79d7f",
   "metadata": {},
   "source": [
    "1. LLMChain\n",
    "2. SequentialChain\n",
    "3. ChatOpenAI \n",
    "4. ConversationBufferMemory \n",
    "4. FileChatMessageHistory\n",
    "4. FileChatMessageHistoryVerbose\n",
    "5. ConversationSummaryMemory\n",
    "6. TextLoader\n",
    "7. CharacterTextSplitter\n",
    "8. OpenAIEmbeddings\n",
    "9. Chroma\n",
    "10. Chain_RetrievalQA\n",
    "11. RedundantFilterRetriever (This will import RedundantFilterRetriever class from RedundantFilterRetrCuser.py in C))tomFilters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a8001",
   "metadata": {},
   "source": [
    "### Basic LLM Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cf4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Golden rays of light\n",
      "Warming up the day\n",
      "Sunshine shining bright\n",
      "Chasing all the gray\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# api_key = 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\n",
    "# llm = OpenAI(openai_api_key=api_key)\n",
    "llm = OpenAI()\n",
    "result = llm(\"write a very very short poem about Sun\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8ae83-8196-44e9-9a12-e59553f1a36c",
   "metadata": {},
   "source": [
    "### LLMChain & PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46554bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# %%\n",
    "# Language Model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Input\n",
    "code_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\",\"language\"],\n",
    "    template=\"Write a very short {language} function that will {task}.\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "code_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=code_prompt,\n",
    "    output_key=\"code\"\n",
    ")\n",
    "\n",
    "# Chain Object that takes inputs \n",
    "result = code_chain({\"task\":'Factorial',\"language\":'Python'})\n",
    "print(result[\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589c7db-1eea-414d-bf34-581dbe3944fd",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">1. LLMChain</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Note: This will cause an error (if you run it in Jupyter notebook or VSCode Interactive Notebook without these command-line arguments) because argparse expects these arguments to be passed in from the command line.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"return a list of numbers\")\n",
    "parser.add_argument(\"--language\", default=\"python\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# %%\n",
    "# Language Model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Input\n",
    "code_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\",\"language\"],\n",
    "    template=\"Write a very short {language} function that will {task}.\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "code_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=code_prompt,\n",
    "    output_key=\"code\"\n",
    ")\n",
    "\n",
    "# Chain Object that takes inputs \n",
    "result = code_chain({\n",
    "    \"task\": args.task,\n",
    "    \"language\": args.language\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb494f-0f55-4a2b-85e7-3f7bbd1fcdbd",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">2. Sequential Chain</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a46f1-9247-4530-b550-37bdae181cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain,SequentialChain\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# Note: This will cause an error (if you run it in Jupyter notebook or VSCode Interactive Notebook without these command-line arguments) because argparse expects these arguments to be passed in from the command line.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"return a list of numbers\")\n",
    "parser.add_argument(\"--language\", default=\"python\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# %%\n",
    "# Chain A: Code Chain\n",
    "# OpenAI Function that does http request on the OpenAI server\n",
    "llm = OpenAI()\n",
    "\n",
    "# Input\n",
    "code_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\",\"language\"],\n",
    "    template=\"Write a very short {language} function that will {task}.\"\n",
    ")\n",
    "\n",
    "# Chain A\n",
    "code_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=code_prompt,\n",
    "    output_key=\"code\"\n",
    ")\n",
    "\n",
    "# Chain B: Test Chain\n",
    "test_prompt = PromptTemplate(\n",
    "    input_variables=[\"language\",\"code\"],\n",
    "    template=\"Write a test for the following code:\\n{code} in {language}\"\n",
    ")\n",
    "\n",
    "test_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=test_prompt,\n",
    "    output_key=\"test\"\n",
    ")\n",
    "\n",
    "# Chaining Chain A and Chain B in a Sequential manner\n",
    "chain = SequentialChain(\n",
    "    chains = [code_chain,test_chain],\n",
    "    input_variables = [\"task\",\"language\"],\n",
    "    output_variables = [\"code\",\"test\"]\n",
    ")\n",
    "result = chain({\n",
    "    \"task\": args.task,\n",
    "    \"language\": args.language\n",
    "})\n",
    "print(f\">>>>> GENERATED CODE  <<<<<<\\n{result['code']}\")\n",
    "print(f\">>>>> GENERATED TEST  <<<<<<\\n{result['test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8350f7a-ff13-4231-a56f-59ce9f3f1e51",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">3. ChatOpenAI</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b407315-b71d-4845-bcb5-06b5f893718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# Chain A: Code Chain\n",
    "# OpenAI Function that does http request on the OpenAI server\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Input\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages = [\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('>>> User Message: ')\n",
    "    if content == 'break':\n",
    "        break\n",
    "    else:\n",
    "        result = chain(\n",
    "            {\n",
    "            \"content\": content\n",
    "        }\n",
    "    )\n",
    "    print(f\">>> AI Message: {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f7a6d-434f-45d8-8907-d861044292f8",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">4. ConversationBufferMemory</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532e02a-1065-4c12-87b7-6f83eccc0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# LLM / Interface\n",
    "chatllm = ChatOpenAI()\n",
    "\n",
    "# Input\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\",\"messages\"],\n",
    "    messages= [\n",
    "        # specifically look for 'messages' key in the modified input (i.e., with added 'messages' key), \n",
    "        # so that memory placeholder can expand the messages key value in such a way that LLM can easily understand\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # Can be any key name, need not necessarily be messages\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# New key called 'messages' will be inserted with every input\n",
    "# ConversationBufferMemory can store a history but it can't store history in a 'file', so if you exit the\n",
    "# program everything that is stored in RAM is lost, but FileChatMessageHistory stores in a json file.\n",
    "memory = ConversationBufferMemory(memory_key=\"messages\",return_messages=True)\n",
    "\n",
    "# Chain takes Input and LLM\n",
    "chain = LLMChain(\n",
    "    llm=chatllm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Type break or Ctrl + C to interrupt the loop\n",
    "while True:\n",
    "    content = input(\">> \")\n",
    "    if content=='break':\n",
    "        break\n",
    "    else:\n",
    "        result = chain({\"content\":content})\n",
    "        # Output\n",
    "        print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773906c-8928-42c1-85c8-88ed275f706c",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">4. FileChatMessageHistory</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14241ca-79fc-4175-a825-0e7b9c5c1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory,FileChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# LLM / Interface\n",
    "chatllm = ChatOpenAI()\n",
    "\n",
    "# Input\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\",\"messages\"],\n",
    "    messages= [\n",
    "        # specifically look for 'messages' key in the modified input (i.e., with added 'messages' key), \n",
    "        # so that memory placeholder can expand the messages key value in such a way that LLM can easily understand\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # Can be any key name, need not necessarily be messages\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# New key called 'messages' will be inserted with every input\n",
    "# FileChatMessageHistory is a Class\n",
    "# ConversationBufferMemory can't store history in a file but FileChatMessageHistory does. Even if you exit\n",
    "# the program all the history will be restored\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=FileChatMessageHistory(\"messages.json\"),\n",
    "    memory_key=\"messages\",\n",
    "    return_messages=True)\n",
    "\n",
    "# Chain takes Input and LLM\n",
    "chain = LLMChain(\n",
    "    llm=chatllm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Type break or Ctrl + C to interrupt the loop\n",
    "while True:\n",
    "    content = input(\">> \")\n",
    "    if content=='break':\n",
    "        break\n",
    "    else:\n",
    "        result = chain({\"content\":content})\n",
    "        # Output\n",
    "        print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa859b-eb93-47d9-b154-c7c31750f2d9",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">4. FileChatMessageHistoryVerbose</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10c300-445c-4b7e-8c15-32fbc8aff002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory,FileChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# LLM / Interface\n",
    "chatllm = ChatOpenAI(verbose=True)\n",
    "\n",
    "# Input\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\",\"messages\"],\n",
    "    messages= [\n",
    "        # specifically look for 'messages' key in the modified input (i.e., with added 'messages' key), \n",
    "        # so that memory placeholder can expand the messages key value in such a way that LLM can easily understand\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # Can be any key name, need not necessarily be messages\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# New key called 'messages' will be inserted with every input\n",
    "# FileChatMessageHistory is a Class\n",
    "# ConversationBufferMemory can't store history in a file but FileChatMessageHistory does. Even if you exit\n",
    "# the program all the history will be restored\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=FileChatMessageHistory(\"messages.json\"),\n",
    "    memory_key=\"messages\",\n",
    "    return_messages=True)\n",
    "\n",
    "# Chain takes Input and LLM\n",
    "chain = LLMChain(\n",
    "    llm=chatllm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ctrl + C to interrupt the loop\n",
    "while True:\n",
    "    content = input(\">> \")\n",
    "    if content=='break':\n",
    "        break\n",
    "    else:\n",
    "        result = chain({\"content\":content})\n",
    "        # Output\n",
    "        print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76f132-0512-4557-99b4-71f91544b197",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">5. ConversationSummaryMemory</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f060dba-bc2c-42e6-b811-fca99189977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# LLM / Interface\n",
    "chatllm = ChatOpenAI(verbose=True) # verbose=True (optional)\n",
    "\n",
    "# Input\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\",\"messages\"],\n",
    "    messages= [\n",
    "        # specifically look for 'messages' key in the modified input (i.e., with added 'messages' key), \n",
    "        # so that memory placeholder can expand the messages key value in such a way that LLM can easily understand\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # Can be any key name, need not necessarily be messages\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# New key called 'messages' will be inserted with every input\n",
    "# 'System' key is automatically added and passes the Summary prompt of 'FileChatMessageHistory' to the LLM defined in ConversationSummaryMemory Class \n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=chatllm, # You can pass any summarization LLM, need not necessarily same LLM that you are using for final response\n",
    "    memory_key=\"messages\",\n",
    "    return_messages=True)\n",
    "\n",
    "# Chain takes Input and LLM\n",
    "chain = LLMChain(\n",
    "    llm=chatllm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True # verbose=True optional\n",
    ")\n",
    "\n",
    "# Ctrl + C to interrupt the loop\n",
    "while True:\n",
    "    content = input(\">> \")\n",
    "    if content=='break':\n",
    "        break\n",
    "    else:\n",
    "        result = chain({\"content\":content})\n",
    "        # Output\n",
    "        print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fda2a-a44d-4a0f-b12f-40fe827e852c",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">6. TextLoader</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa150b5-6b42-4804-94e4-b215c1a10fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain.document_loaders import TextLoader\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "docs = loader.load()\n",
    "# docs = TextLoader(\"facts.txt\").load()\n",
    "print(docs)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66716617-e724-4ccf-9e0b-01cb8b0817f4",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">7. CharacterTextSplitter</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec393347-dfa5-47ac-9aaf-b9806ef79518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "# Loader and Splitter\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02813150-8262-4210-beeb-181f0ee58613",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">8. OpenAIEmbeddings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e541bfc-2de2-4f5b-a726-568b5c30e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "#  Embeddings. Embed the chunks using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeds = embeddings.embed_query(\"Hi There\")\n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5448c-15ff-4cbe-8ea1-6f86ab2b1f92",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">9. Chroma</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e3161-124f-4464-b753-bdcaadb01cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma \n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "#  Embeddings. Embed the chunks using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Loader and Splitter\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    " \n",
    "# Creating an instance of Chromadb\n",
    "# All chunks will be converted to embeddings in a single request and stored in a vector store\n",
    "db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"emb\"\n",
    ")\n",
    "#  similarity_search_with_score it gives scores\n",
    "results = db.similarity_search_with_score(\"What is an interesting fact about the English language?\",\n",
    "                                          k=5) # k = Return Top N results\n",
    "for result in results:\n",
    "    print()\n",
    "    print(f'Similarity Search Score: {result[1]}') # Less Score means less Distance means More Similarity\n",
    "    print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5febf-fdba-44b3-92b9-6c66e51accad",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">10. RetrievalQA Chain</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501189c8-0482-41dc-abff-8433942703bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "# %%\n",
    "load_dotenv()\n",
    "#  Embeddings. Embed the chunks using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Location of the Vector Store and Embedding that it needs to use for Retrevial\n",
    "db = Chroma(\n",
    "    persist_directory=\"emb\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Retriever\n",
    "# A Retriever is an object that can take in a string and return some relevant documents.\n",
    "# To be a \"Retriever\", the object must have a method called \"get_relevant_documents\" that takes a string and returns a list of documents.\n",
    "# Look up (it'not just passive look up it's finding a similarity) Query against the Vector Store for Similar results. We call it as Retriever or RetrieverQA to be specific\n",
    "# It's Modular. You can you any retriever that might have slightly different functions to find documents\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# which LLM to use by Chain\n",
    "chatllm = ChatOpenAI()\n",
    "\n",
    "# RetrievalQA Chain: LangChain has a tool that basically wraps up this entire flow. It's going to take in our vector store. It's going to encode or generate some embeddings for an incoming user question or a query. Find some relevant documents, inject / 'stuff' them into a system message prompt template, take the user's question and put it into human message prompt template, and then feed the entire thing into an LLM Chain for us. This construct is called a retrieval chain, or in the source code it's called really just a retrieval.\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chatllm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "# We will pass the input to the retriever here in the chain because always chain takes the input and re-routes the input to the retriever.\n",
    "result = chain.run(\"What is an interesting fact about the English language?\")\n",
    "print(result)\n",
    "\n",
    "# The main problem with above code is it doesn't remove any duplicate records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d8467-d291-49bf-9811-0323a063ddbb",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">11. RedundantFilterRetriever (This will import RedundantFilterRetriever class from RedundantFilterRetrCuser.py in CustomFilters)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce099fe9-eeb1-4577-a5b6-b58004c81eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from CustomFilters.RedundantFilterRetriever import RedundantFilterRetriever\n",
    "# from Folder.File import Class\n",
    "from dotenv import load_dotenv\n",
    "# import langchain\n",
    "# \n",
    "# langchain.debug = True # to Turn ON Debugging Mode\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "#  Embeddings. Embed the chunks using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Location of the Vector Store and Embedding that it needs to use for Retrevial\n",
    "db = Chroma(\n",
    "    persist_directory=\"emb\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Retriever\n",
    "# A Retriever is an object that can take in a string and return some relevant documents.\n",
    "# To be a \"Retriever\", the object must have a method called \"get_relevant_documents\" that takes a string and returns a list of documents.\n",
    "# Look up (it'not just passive look up it's finding a similarity) Query against the Vector Store for Similar results. We call it as Retriever or RetrieverQA to be specific\n",
    "# It's Modular. You can you any retriever that might have slightly different functions to find documents\n",
    "retriever = RedundantFilterRetriever(\n",
    "    embeddings=embeddings,\n",
    "    chroma=db\n",
    "    )\n",
    "\n",
    "# which LLM to use by Chain\n",
    "chatllm = ChatOpenAI()\n",
    "\n",
    "# RetrievalQA Chain: LangChain has a tool that basically wraps up this entire flow. It's going to take in our vector store. It's going to encode or generate some embeddings for an incoming user question or a query. Find some relevant documents, inject / 'stuff' them into a system message prompt template, take the user's question and put it into human message prompt template, and then feed the entire thing into an LLM Chain for us. This construct is called a retrieval chain, or in the source code it's called really just a retrieval.\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chatllm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "# We will pass the input to the retriever here in the chain because always chain takes the input and re-routes the input to the retriever.\n",
    "result = chain.run(\"What is an interesting fact about the English language?\") # chain.run gives string in 'result' key as output\n",
    "# result = chain(\"What is an interesting fact about the English language?\")  # chain gives dictionary as output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951aaa17-f328-4eb1-9256-6a42fcfe2511",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820776c-c265-4be2-95ec-cd179cd35862",
   "metadata": {},
   "source": [
    "Notes: OneNote > AI > LangChain > Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af673b-d615-416f-9ce8-b7ffd903df16",
   "metadata": {},
   "source": [
    "##### Defining a Tool (sql.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f89b74-9a96-4e01-998d-d50b19aef465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langchain.tools import Tool\n",
    "\n",
    "connection = sqlite3.connect(\"db.sqlite\")\n",
    "\n",
    "def run_sqlite_query(query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "run_query_tool = Tool.from_function(\n",
    "    name=\"run_sqlite_query\",\n",
    "    description=\"Run a sqlite query\",\n",
    "    func=run_sqlite_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03c6a6-d14f-49bf-9cd9-3452efa47bc2",
   "metadata": {},
   "source": [
    "##### Defining an Agent and AgentExecutor (main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba910f1-8c4f-4727-89d2-ec2e36b86041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"How many users are there in the database?\")\n",
    "# agent_executor(\"How many users have provided a shipping address?\") # This will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522a520-fbfc-49d5-bbd1-1c498284d3c6",
   "metadata": {},
   "source": [
    "#### Addressing Shortcomings in ChatGPTs Assumptions\n",
    "Shortcomings: ChatGPT is assuming table names and column names which may or may not be the same in actual database. So, ChatGPT query is not compatible with our actual database at all and we will end up with getting error.\n",
    "\n",
    "Solution: We need to do a better job of getting ChatGPT to try to investigate our database a little bit. Or maybe give it some more information just right up front,  give it enough information to understand what tables exist and what columns are in each of those tables. And then whenever we ask it a question that's a little bit more complex like this one right here (How many users have provided a shipping address?). We want ChatGPT to use that knowledge to formulate a query, rather than just assuming the table's name and it has a column called shipping address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cf548-6912-4bc8-be3d-3cf3fdb2251b",
   "metadata": {},
   "source": [
    "##### Strategy 1: Doing a little bit of Error Handling (especially Operational Errors)\n",
    "Rather than just throwing the error and exiting our program, which is what's happening right now (when we gave 'How many users have provided a shipping address?'), we're going to instead capture that error message. We're going to get the error message itself, like the actual text that describes exactly what just went wrong. We're then going to take that error message and send it off to ChatGPT. So we're gonna say something like the following error occurred while running the query you gave me, and then we'll put in exactly what the error message was. So we're gonna send that off to ChatGPT and the hope here, this is our hope. It's not actually going to, like I said, solve the entire problem, but our hope is that ChatGPT is going to realize, \"Oh, okay, a column called shipping address doesn't actually exist, I'll try again.\" So hopefully it will send us a follow-up query and ideally this one will actually work. Now, once again, I wanna tell you, this is not gonna solve the entire problem, but this is just kind of step one. It's to help ChatGPT understand when something goes wrong and give it the chance or the opportunity to fix things up. To implement all this, we're going to make an update to our tool function. The actual function that takes the query from ChatGPT and executes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8d2c2-c7c8-499b-810f-b97a1a819f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql.py\n",
    "import sqlite3\n",
    "from langchain.tools import Tool\n",
    "\n",
    "connection = sqlite3.connect(\"db.sqlite\")\n",
    "\n",
    "def run_sqlite_query(query):\n",
    "    cursor = connection.cursor()\n",
    "    try:# Error Handling\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "    except sqlite3.OperationalError as err:\n",
    "        return f'The following error occurred: {str(err)}'\n",
    "\n",
    "run_query_tool = Tool.from_function(\n",
    "    name=\"run_sqlite_query\",\n",
    "    description=\"Run a SQLite query\",\n",
    "    func=run_sqlite_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39ef21-0178-461f-ae6d-8e90dd240778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"How many users have provided a shipping address?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb103941-a24b-4214-8c1c-9df40c2d4c83",
   "metadata": {},
   "source": [
    "##### Adding in Error Handling is definitely good because it gives ChatGPT the ability to follow up and try to fix an error, but it still is not really enough. Now we need to help ChatGPT understand the structure of our database just a little bit more. So we're going to take a look at two additional techniques that we're going to use to make ChatGPT a lot more robust in answering questions that involve our database. So here's the two additional techniques we're going to use. We're going to make two changes to the initial requests we send off to ChatGPT. First we are going to add in a system message and Second we're going to add in an additional tool.\n",
    "![](../Media/lc_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bbcd5-ec1b-4515-a65e-f1d5ae27d74a",
   "metadata": {},
   "source": [
    "##### First we are going to add in a system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ddf22-e4d6-4d71-b8fa-b9a3a498acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql.py\n",
    "import sqlite3\n",
    "from langchain.tools import Tool\n",
    "\n",
    "connection = sqlite3.connect(\"db.sqlite\")\n",
    "\n",
    "def tables_str():\n",
    "    conn = sqlite3.connect('db.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"Select name from sqlite_master where type = 'table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    return '\\n'.join(row[0] for row in tables if row[0] != None)\n",
    "    \n",
    "def run_sqlite_query(query):\n",
    "    cursor = connection.cursor()\n",
    "    try:# Error Handling\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "    except sqlite3.OperationalError as err:\n",
    "        return f'The following error occurred: {str(err)}'\n",
    "\n",
    "run_query_tool = Tool.from_function(\n",
    "    name=\"run_sqlite_query\",\n",
    "    description=\"Run a SQLite query\",\n",
    "    func=run_sqlite_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd2924-bda8-4990-9bb7-0f3a2ee39b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool, tables_str\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "tables = tables_str()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an AI and you have access to database.\\n{tables}\"), # It's a Static Message so no need to use System Message Prompt Template\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"How many users have provided a shipping address?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cde92-10dd-4cbc-bfb0-e3827051df25",
   "metadata": {},
   "source": [
    "Observation: Still AI is not using correct column from a table in a given list of tables in System Message. So we need to give a tool to ChatGPT to describe a table from a given list of tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b8a84-204d-4954-a19c-f62583e0cc86",
   "metadata": {},
   "source": [
    "##### Second we're going to add in an additional describe tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33e1a9-505f-4a92-bc58-8c27b9728c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql.py\n",
    "import sqlite3\n",
    "from langchain.tools import Tool\n",
    "\n",
    "connection = sqlite3.connect(\"db.sqlite\")\n",
    "\n",
    "def tables_str():\n",
    "    conn = sqlite3.connect('db.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"Select name from sqlite_master where type = 'table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    return '\\n'.join(row[0] for row in tables if row[0] != None)\n",
    "    \n",
    "def run_sqlite_query(query):\n",
    "    cursor = connection.cursor()\n",
    "    try:# Error Handling\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "    except sqlite3.OperationalError as err:\n",
    "        return f'The following error occurred: {str(err)}'\n",
    "\n",
    "run_query_tool = Tool.from_function(\n",
    "    name=\"run_sqlite_query\",\n",
    "    description=\"Run a SQLite query\",\n",
    "    func=run_sqlite_query\n",
    ")\n",
    "\n",
    "def describe_tables(table_names):\n",
    "    tables_list = table_names.split('\\n')\n",
    "    tables = \"('\" + \"', '\".join(tables_list) + \"')\"\n",
    "    cursor = connection.cursor()\n",
    "    rows = cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' and name IN {tables};\")\n",
    "    return '\\n'.join(row[0] for row in rows if row[0] is not None)\n",
    "\n",
    "describe_tables_tool = Tool.from_function(\n",
    "    name=\"describe_tables\",\n",
    "    description=\"Given a list of table names, returns the schema of those tables\",\n",
    "    func=describe_tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5421b-32eb-4e46-b342-40502a63ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool, tables_str, describe_tables_tool\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "tables = tables_str()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an AI and you have access to database.\\n{tables}\"), # It's a Static Message so no need to use System Message Prompt Template\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool,describe_tables_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"How many users have provided a shipping address?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63ac92-24b0-429f-823e-1a0bd2978be8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Observation: Even though we've added in the system message and this additional tool, we're gonna very quickly realize that it's really hard to get ChatGPT to make use of that new tool. ChatGPT luckily after several failed attempts used our new describe tool in above but be aware, right away, that unfortunately this is not gonna solve all of our problems. Sometimes ChatGPT thinks that it just knows what's best for answering some of these questions and it's going to refuse to use some tools. So I'll show you some ways we're going to get around that. One way is to insist ChatGPT to use our describe tool i.e., by 'Being Direct with System Messages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7c0a9-27de-4beb-8d93-be2032765af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool, tables_str, describe_tables_tool\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "tables = tables_str()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an AI that has access to a SQLite database.\\n\"\n",
    "            f\"The database has tables of: {tables}\\n\"\n",
    "            \"Do not make any assumptions about what tables exist \"\n",
    "            \"or what columns exist. Instead, use the 'describe_tables' function\"), # It's a Static Message so no need to use System Message Prompt Template\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool,describe_tables_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"How many users have provided a shipping address?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab1970-cde6-40c0-9373-7dbdb094035f",
   "metadata": {},
   "source": [
    "* https://www.udemy.com/course/chatgpt-and-langchain-the-complete-developers-masterclass/learn/lecture/40261308#notes\n",
    "* Even though our program is now working, one more little optimization we can make to better help ChatGPT understand is, how to make use of the tools we have provided to ChatGPT. \n",
    "* \n",
    "LangChain rather than putting in the actual name of the argument that our defined function expects, something like \"query\" or \"table names\" or something like that, Langchain, unfortunately, puts in argument names like \"__arg1\". This is the default behavior of langchain, rather than putting in actual descriptive argument names, that would help chatGPT better understand how to make use of your tool, it just throws in things like \"__arg. \n",
    "\n",
    "* So the very small optimization we can make is to customize these names. So to make sure that we get in names like \"query\", If you put in a name here, something like gibberish (like __arg1), well chatGPT is not going to be able to make use of that very well. But if you put in a name like \"query\" then chatGPT is gonna understand very plainly, oh, okay, the first argument is going to be the query that I want to run.  \n",
    "\n",
    "* Pydantic, this is a library that allows us to annotate different classes inside of a python class, and just kind of more clearly describe what kinds of data we expect that class to receive as attributes (Instead of correcting LangChain, we are setting expectations to ChatGPT to give properties in a particular name or particular datatype etc). This base model class is what adds on that extra little bit behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e672abd-8d4a-450e-82ea-d8c8affdbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql.py\n",
    "import sqlite3\n",
    "from langchain.tools import Tool\n",
    "from typing import List\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "connection = sqlite3.connect(\"db.sqlite\")\n",
    "\n",
    "def tables_str():\n",
    "    conn = sqlite3.connect('db.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"Select name from sqlite_master where type = 'table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    return '\\n'.join(row[0] for row in tables if row[0] != None)\n",
    "    \n",
    "def run_sqlite_query(query):\n",
    "    cursor = connection.cursor()\n",
    "    try:# Error Handling\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "    except sqlite3.OperationalError as err:\n",
    "        return f'The following error occurred: {str(err)}'\n",
    "\n",
    "# By making this class, we have created some kind of record inside of our program. It says essentially, if you want to be a class of RunQueryArgsSchema, you must provide a 'query' attribute that is a string. We then provided that off to our tool under the keyword argument args_schema. Langchain internally is going to use this information to better describe the different arguments that ChatGPT should be providing to our tool. So in this case, it's gonna tell ChatGPT, \"Okay, if you want to make use of this tool and if you want to use this function, you must provide an argument called query, and it's supposed to be a string.\"\n",
    "class RunQueryArgsSchema(BaseModel):\n",
    "    query: str\n",
    "\n",
    "run_query_tool = Tool.from_function(\n",
    "    name=\"run_sqlite_query\",\n",
    "    description=\"Run a SQLite query\",\n",
    "    func=run_sqlite_query\n",
    ")\n",
    "\n",
    "def describe_tables(table_names):\n",
    "    # tables_list = table_names.split('\\n')    # This is not required if you define a Pydantic class DescribeTablesArgsSchema\n",
    "    tables = \"('\" + \"', '\".join(table_names) + \"')\"\n",
    "    cursor = connection.cursor()\n",
    "    rows = cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' and name IN {tables};\")\n",
    "    return '\\n'.join(row[0] for row in rows if row[0] is not None)\n",
    "\n",
    "class DescribeTablesArgsSchema(BaseModel):\n",
    "    table_names: List[str]\n",
    "    \n",
    "describe_tables_tool = Tool.from_function(\n",
    "    name=\"describe_tables\",\n",
    "    description=\"Given a list of table names, returns the schema of those tables\",\n",
    "    func=describe_tables,\n",
    "    args_schema=DescribeTablesArgsSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca2c2a-d47f-4ba4-a13c-86d2c489b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool, tables_str, describe_tables_tool\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "# import langchain\n",
    "\n",
    "# langchain.debug = True\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI()\n",
    "tables = tables_str()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an AI that has access to a SQLite database.\\n\"\n",
    "            f\"The database has tables of: {tables}\\n\"\n",
    "            \"Do not make any assumptions about what tables exist \"\n",
    "            \"or what columns exist. Instead, use the 'describe_tables' function\"), # It's a Static Message so no need to use System Message Prompt Template\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool,describe_tables_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"How many users have provided a shipping address?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d86f6f-5c86-4c50-93e7-e20a3a776cd5",
   "metadata": {},
   "source": [
    "Why are we using Structured Tool here instead of Tool? \n",
    "* Well, it's a very, very simple reason because of some legacy decisions in a lang chain library, whenever you create a class or a tool out of the Tool class, as we did back inside of our sql.py file, This Tool class (langchain.tools.Tool) can only use functions that receive a single argument. Sounds silly, I know, but it's due to a lot of legacy issues inside of the lang chain code base. If you want to make a tool that's going to receive multiple different arguments, we're going to instead use this structured tool class instead. \n",
    "* And very simply, this StructuredTool can receive multiple different arguments. So it is a perfect fit for our use case where we want to receive one and two separate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb360f4-3476-449f-80d8-4bf2367af42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report.py\n",
    "from pydantic.v1 import BaseModel\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "def write_report(filename, html):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "class WriteReportArgsSchema(BaseModel):\n",
    "    filename: str\n",
    "    html: str\n",
    "    \n",
    "write_report_tool = StructuredTool.from_function(\n",
    "    name=\"write_report\",\n",
    "    description=\"Write an HTML file to disk. Use this tool whenever someone asks for a report.\",\n",
    "    func=write_report,\n",
    "    args_schema=WriteReportArgsSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ec7fa-87f5-44f3-9837-b49d1c6fb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool, tables_str, describe_tables_tool\n",
    "from tools.report import write_report_tool\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "# import langchain\n",
    "\n",
    "# langchain.debug = True\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI()\n",
    "tables = tables_str()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an AI that has access to a SQLite database.\\n\"\n",
    "            f\"The database has tables of: {tables}\\n\"\n",
    "            \"Do not make any assumptions about what tables exist \"\n",
    "            \"or what columns exist. Instead, use the 'describe_tables' function\"), # It's a Static Message so no need to use System Message Prompt Template\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool, describe_tables_tool, write_report_tool]\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor(\"Summarize the top 5 most popular products. Write the results to a report file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c8acb-9fda-4d30-9852-d871e390a6c1",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Brown\">Memory in Agents</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461d0c5-3c3c-4a10-8b5b-ceab630b6857",
   "metadata": {},
   "source": [
    "Notes: OneNote > AI > LangChain > Memory in Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b6d7b-6c7c-4d60-a9eb-f4aab9f3cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import OpenAIFunctionsAgent,AgentExecutor\n",
    "from tools.sql import run_query_tool, tables_str, describe_tables_tool\n",
    "from tools.report import write_report_tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "# import langchain\n",
    "\n",
    "# langchain.debug = True\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI()\n",
    "tables = tables_str()\n",
    "# memory_key is chat_history. And remember, we also need to add in that other keyword argument of return_messages true. And we're putting that in to make sure that the memory is going to return a list of memory objects. So when it says return_messages=True, it means return or give us back the list of messages as message objects as opposed to a bunch of strings. We would use that list of strings if we were using a completion-based language model (say if we set return_messages=False). And we aren't, we are using ChatGPT here, which is message/chat-based.  \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an AI that has access to a SQLite database.\\n\"\n",
    "            f\"The database has tables of: {tables}\\n\"\n",
    "            \"Do not make any assumptions about what tables exist \"\n",
    "            \"or what columns exist. Instead, use the 'describe_tables' function\"), # It's a Static Message so no need to use System Message Prompt Template\n",
    "        # We need to add in a MessagesPlaceholder for the chat_history as well. So this is going to be the thing that tries to find the stored list of messages (with variable_name=chat_history) and it's gonna kind of explode and convert into all the previous messages that were exchanged. So we'll put in another MessagesPlaceholder with a variable name of chat_history.\n",
    "        # And I'm putting it right here specifically because I want to add it before any brand new human message that we add in.\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\") # Remember, the goal of MessagesPlaceholder is to kind of take in an input variable and then explode or expand into a new list of messages. Agent Scratch Pad is exactly same as MessagesPlaceholder(variable_name=\"messages\") in ConversationSummaryMemory (LangChainBasics.py).\n",
    "    ]\n",
    "    )\n",
    "\n",
    "tools = [run_query_tool, describe_tables_tool, write_report_tool]\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    # To use the ConversationBufferMemory as this Memory object that we just created, we're going to add it into our agent executor because that is the thing that needs to remember this list of messages. \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "agent_executor(\n",
    "    \"How many orders are there? Write the result to an html report.\"\n",
    ")\n",
    "\n",
    "# This is going to use ConversationBufferMemory and repeat the process.\n",
    "agent_executor(\n",
    "    \"Repeat the exact same process for users.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

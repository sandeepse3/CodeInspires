{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Updated Date: 30 April 2021\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "print(\"Last Updated Date:\", today.strftime(\"%d %B %Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>[Scikit Learn](#Scikit_Learn)\n",
    "<br>[SciKitLearn Cheat Sheet](#SciKitLearn_Cheat_Sheet)\n",
    "<br>[Scaling our data and splitting our data into Train and Test Datasets](#Scaling_our_data_and_splitting_our_data_into_Train_and_Test_Datasets)\n",
    "<br>[Nomalization of our Data / Scaling our Data](#Nomalization_of_our_Data_or_Scaling_our_Data)\n",
    "<br>[Splitting our Data into Train and Test Datasets / Sets](#Splitting_our_Data_into_Train_and_Test_Datasets_or_Sets)\n",
    "<br>[Now we do a train test split using an existing train-test-split library](#Now_we_do_a_train_test_split_using_an_existing_train-test-split_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>[SciPy](#SciPy)\n",
    "<br>[Compute The Nth Derivate Of A Function](#Compute_The_Nth_Derivate_Of_A_Function)\n",
    "<br>[Permutation And Combinations](#Permutation_And_Combinations)\n",
    "<br>[Linear Algebra](#Linear_Algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Scikit_Learn'></a>Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sci Kit Learn is actually its own machine learning library for Python and it's one of most popular libraries out there but it doesn't support the deep neural networks that pytorch / tensorflow can do so. Which is why we're not really going to be using it in this course, if you're interested in some of those other scikit learn machine learning model methods, you can check out that Python for data science and machine learning bootcamp course\n",
    "* SciKit Learn is a Pythonâ€™s Machine Learning Library / Module and contains machine learning models. We are going to use SciKit * Learn mainly for pre-processing. In preprocessing, specifically for two things\n",
    "    * 1) Scaling our data (Normalization of data)\n",
    "    * 2) Splitting our data into train and test datasets / sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            # Questions\n",
    "# import libraries?\n",
    "# what does MinMaxScaler does?\n",
    "# How do you fit and transform data? and what is purpose of Normalization\n",
    "# How do split data into training and test data and what is the purpose of splitting the data\n",
    "# What Dataframe in pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='SciKitLearn_Cheat_Sheet'></a>SciKitLearn Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar_model = MinMaxScaler()\n",
    "scalar_model.fit_transform(data) (or in two steps scalar_model.fit(data) and scalar_model.transform(data))\n",
    "mydata = np.arange(0,100).reshape(10,10)\n",
    "df = pd.DataFrame(mydata,columns = ['f1','f2','f3','label'])\n",
    "X = df[['f1','f2','f3']]\n",
    "y = df['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) #Just type train_test_split and enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoJQII7nyhjI"
   },
   "source": [
    "- It provides simple and efficient tools for pre-processing and predictive modeling\n",
    "<br>![](./Media/Py2.png)\n",
    "\n",
    "***Steps to build a model in scikit-learn.***\n",
    "1. Import the model\n",
    "2. Prepare the data set\n",
    "3. Separate the independent and target variables.\n",
    "4. Create an object of the model\n",
    "5. Fit the model with the data\n",
    "6. Use the model to predict target.\n",
    "\n",
    "***Learn more about the scikit-learn here: https://scikit-learn.org/stable/index.html***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwk5CRntyhjL"
   },
   "outputs": [],
   "source": [
    "# import the scikit-learn library\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y87SJsx8yhjT",
    "outputId": "b38c6a3f-c44b-45f3-bc6e-350a3497c8ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the version \n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELQTIy0Uyhja"
   },
   "source": [
    "- ***We have seen in the pandas notebook, that we have some missing values in out data.***\n",
    "- ***We will impute those missing values using the scikit-learn Imputer.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdNfOlcXyhjd",
    "outputId": "c9db4d7f-ebd5-492a-87d2-5e2245210f51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier                 0\n",
       "Item_Weight                  1463\n",
       "Item_Fat_Content                0\n",
       "Item_Visibility                 0\n",
       "Item_Type                       0\n",
       "Item_MRP                        0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Size                  2410\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Type                     0\n",
       "Item_Outlet_Sales               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data set and check for thre null values\n",
    "import pandas as pd\n",
    "data = pd.read_csv('big_mart_sales.csv')\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzPDG1piyhjk"
   },
   "outputs": [],
   "source": [
    "# import the SimpleImputer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MEmKt_XPyhjp"
   },
   "source": [
    "---\n",
    "\n",
    "- For imputing the missing values, we will use `SimpleImputer`.\n",
    "- First we will create an object of the Imputer and define the strategy.\n",
    "- We will impute the Item_Weight by `mean` value and Outlet_Size by `most_fequent` value.\n",
    "- Fit the objects with the data.\n",
    "- Transform the data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3A-KoFkyhjr"
   },
   "outputs": [],
   "source": [
    "# create the object of the imputer for Item_Weight and Outlet_Size\n",
    "impute_weight = SimpleImputer(strategy= 'mean')\n",
    "impute_size   = SimpleImputer(strategy= 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nGOCwtUyhjz"
   },
   "outputs": [],
   "source": [
    "# fit the Item_Weight imputer with the data and transform\n",
    "impute_weight.fit(data[['Item_Weight']])\n",
    "data.Item_Weight = impute_weight.transform(data[['Item_Weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNuD2XmVyhj4"
   },
   "outputs": [],
   "source": [
    "# fit the Outlet_Size imputer with the data and transform\n",
    "impute_size.fit(data[['Outlet_Size']])\n",
    "data.Outlet_Size = impute_size.transform(data[['Outlet_Size']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4C-1F39yhj9",
    "outputId": "1b5890e1-f3d2-418d-a3b3-0be283b81c15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier              0\n",
       "Item_Weight                  0\n",
       "Item_Fat_Content             0\n",
       "Item_Visibility              0\n",
       "Item_Type                    0\n",
       "Item_MRP                     0\n",
       "Outlet_Identifier            0\n",
       "Outlet_Establishment_Year    0\n",
       "Outlet_Size                  0\n",
       "Outlet_Location_Type         0\n",
       "Outlet_Type                  0\n",
       "Item_Outlet_Sales            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the null values.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gJYHlgsyhkD"
   },
   "source": [
    "- ***Now, after the preprocessing step, we separate the independent and target variable and pass the data to the model object to train the model.***\n",
    "---\n",
    "\n",
    "- ***If we have a problem in which we have to identify the category of an object based on some features. For example whether the given picture is of a cat or a dog. These are `classification problems`.***\n",
    "- ***Or, if we have to identify a continous attribute like predicting sales based on some features. These are `Regression Problems`***\n",
    "\n",
    "---\n",
    "\n",
    "***`SCIKIT-LEARN` has tools which will help you build Regression, Classification models and many others.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0gcpZ-XyhkG"
   },
   "outputs": [],
   "source": [
    "# some of the very basic models scikit learn has.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVl66P1FyhkL"
   },
   "source": [
    "---\n",
    "\n",
    "After we have build the model now whenever new data points are added to the existing data, we need to perform the same preprocessing steps again before we can use the model to make predictions. This becomes a tedious and time consuming process!\n",
    "\n",
    "So, scikit-learn provides tools to create a pipeline of all those steps that will make your work a lot more easier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xV7B1O-yhkM"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Scaling_our_data_and_splitting_our_data_into_Train_and_Test_Datasets'></a>Scaling our data and splitting our data into Train and Test Datasets\n",
    "* We're going to 'fit your training data' and then 'transform your training data' and then 'transform your test data'. \n",
    "* And the reason for that is because you don't really want to cheat by fitting to your test data as well as your training data because you don't want to assume that you're going to know what your test data is going to look like. So typically you fit to your training data and then you transform to your test data and training data. But the model itself has only been fitted to your training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Nomalization_of_our_Data_or_Scaling_our_Data'></a>Nomalization of our Data / Scaling our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(0,100,(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 88],\n",
       "       [19, 48],\n",
       "       [83, 35],\n",
       "       [87, 35],\n",
       "       [19, 93],\n",
       "       [67, 33],\n",
       "       [ 2, 79],\n",
       "       [58, 55],\n",
       "       [54, 65],\n",
       "       [58, 14]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If we want to run the Data in the Neural Networks then we have to scale that particular data, we can achieve that using the MinMaxScaler__\n",
    "* from sklearn.preprocessing import MinMaxScaler\n",
    "<br>\n",
    "Note: There are otherways of Normalizing the data but since we are dealing with simple datasets MinMaxScaler is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing.data.MinMaxScaler"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating an instance of MinMaxScaler\n",
    "scaler_model =  MinMaxScaler()  #MinMaxScaler() is class or datatype\n",
    "type(scaler_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_model.fit(data) #It gives the range of values here it is (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47058824, 0.93670886],\n",
       "       [0.2       , 0.43037975],\n",
       "       [0.95294118, 0.26582278],\n",
       "       [1.        , 0.26582278],\n",
       "       [0.2       , 1.        ],\n",
       "       [0.76470588, 0.24050633],\n",
       "       [0.        , 0.82278481],\n",
       "       [0.65882353, 0.51898734],\n",
       "       [0.61176471, 0.64556962],\n",
       "       [0.65882353, 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_model.transform(data)  #It transforms the actual data into model's version of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scaler_model.fit_transform(data)  **(in a single step instead of two below steps)**\n",
    "* scaler_model.fit(data)\n",
    "* scaler_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47058824, 0.93670886],\n",
       "       [0.2       , 0.43037975],\n",
       "       [0.95294118, 0.26582278],\n",
       "       [1.        , 0.26582278],\n",
       "       [0.2       , 1.        ],\n",
       "       [0.76470588, 0.24050633],\n",
       "       [0.        , 0.82278481],\n",
       "       [0.65882353, 0.51898734],\n",
       "       [0.61176471, 0.64556962],\n",
       "       [0.65882353, 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_model.fit_transform(data)   #Instead of two lines of code, we can complete do it in a single step\n",
    "#There are otherways of Normalizing the data, but since we are dealing with most basic data sets MinMaxScaler() is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Splitting_our_Data_into_Train_and_Test_Datasets_or_Sets'></a>Splitting our Data into Train and Test Datasets / Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = np.random.randint(0,101,(50,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 65,   3,  51,  35],\n",
       "       [ 10,  53,  49, 100],\n",
       "       [ 21,  72,  96,  74],\n",
       "       [ 63,  83,  51,  82],\n",
       "       [  5,  74,   0,  29],\n",
       "       [ 31,  57,  29,  68],\n",
       "       [ 83,  84,   6,  68],\n",
       "       [ 93,  43,   6,  77],\n",
       "       [ 38,  20,  97,  97],\n",
       "       [ 79,  95,   7,   6],\n",
       "       [ 40,  99,  32,  17],\n",
       "       [ 54,  58,  19,  34],\n",
       "       [ 61,  16,  98,  86],\n",
       "       [ 72,  66,   4,  60],\n",
       "       [ 39,  32,  70,  80],\n",
       "       [ 56,  74,  19,   0],\n",
       "       [ 51,  89,  31,  45],\n",
       "       [ 43,  87,  45,  89],\n",
       "       [ 88,  94,  64,  13],\n",
       "       [ 97,  28,  43,  47],\n",
       "       [ 89,  89,  43,  22],\n",
       "       [  9,  35,  30,  82],\n",
       "       [ 22,  70,  71,  73],\n",
       "       [ 40,   5,  73,  25],\n",
       "       [ 47,  96,  39,  75],\n",
       "       [ 31,  13,  86,  34],\n",
       "       [ 44,  28,   2,  86],\n",
       "       [ 90,  25,   1,  10],\n",
       "       [  2,  14,  52,  94],\n",
       "       [ 40,  35,  76,  75],\n",
       "       [ 52,  15,  39,  67],\n",
       "       [ 78,  44,  62,  71],\n",
       "       [ 40,  41,  36,  34],\n",
       "       [ 66,  71,   9,  86],\n",
       "       [ 65,  66,  91,  99],\n",
       "       [ 39,  15,  25,  66],\n",
       "       [ 49,  54,   7,   8],\n",
       "       [ 31,  66,   3,  82],\n",
       "       [ 20,  27,  51,  17],\n",
       "       [ 51,   6,  73,  31],\n",
       "       [ 40,  18,  21,  90],\n",
       "       [  3,   0,  68,  84],\n",
       "       [ 33,  94,  24,  21],\n",
       "       [ 55,  12,  24,  32],\n",
       "       [ 78,  97,  99,  52],\n",
       "       [ 22,  18,  82,   5],\n",
       "       [ 59,  60,  67,  74],\n",
       "       [ 46,  57,  90,  32],\n",
       "       [ 29,  56,  56,   4],\n",
       "       [ 84,  16,  81,  54]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Frames are like tables or excel sheets where you can perform operations on individual columns or indexes. The best example for operations on matrix is given below (where f1,f2,f3 are feature columns and last column is label column)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mydata,columns = ['f1','f2','f3','label']) \n",
    "# 3 features f1,f2,f3 (features / magnitudes / weights) and label (solution)\n",
    "# This is a supervised learning problem. This is exactly supervised learning problem looks like\n",
    "# We are having 3 features and we are trying predict this 'label' (which is nothing but solving a 3 variable linear equation)\n",
    "# 3 variables and we need 3 linear equations to solve them (similary to solve 'n' variables we need 'n' equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>72</td>\n",
       "      <td>96</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>83</td>\n",
       "      <td>51</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>79</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40</td>\n",
       "      <td>99</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>54</td>\n",
       "      <td>58</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>61</td>\n",
       "      <td>16</td>\n",
       "      <td>98</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>72</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>56</td>\n",
       "      <td>74</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>51</td>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43</td>\n",
       "      <td>87</td>\n",
       "      <td>45</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>88</td>\n",
       "      <td>94</td>\n",
       "      <td>64</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>97</td>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>43</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>47</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>86</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>44</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>78</td>\n",
       "      <td>44</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>66</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>91</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>73</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>33</td>\n",
       "      <td>94</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>78</td>\n",
       "      <td>97</td>\n",
       "      <td>99</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>67</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>90</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>29</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>84</td>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1  f2  f3  label\n",
       "0   65   3  51     35\n",
       "1   10  53  49    100\n",
       "2   21  72  96     74\n",
       "3   63  83  51     82\n",
       "4    5  74   0     29\n",
       "5   31  57  29     68\n",
       "6   83  84   6     68\n",
       "7   93  43   6     77\n",
       "8   38  20  97     97\n",
       "9   79  95   7      6\n",
       "10  40  99  32     17\n",
       "11  54  58  19     34\n",
       "12  61  16  98     86\n",
       "13  72  66   4     60\n",
       "14  39  32  70     80\n",
       "15  56  74  19      0\n",
       "16  51  89  31     45\n",
       "17  43  87  45     89\n",
       "18  88  94  64     13\n",
       "19  97  28  43     47\n",
       "20  89  89  43     22\n",
       "21   9  35  30     82\n",
       "22  22  70  71     73\n",
       "23  40   5  73     25\n",
       "24  47  96  39     75\n",
       "25  31  13  86     34\n",
       "26  44  28   2     86\n",
       "27  90  25   1     10\n",
       "28   2  14  52     94\n",
       "29  40  35  76     75\n",
       "30  52  15  39     67\n",
       "31  78  44  62     71\n",
       "32  40  41  36     34\n",
       "33  66  71   9     86\n",
       "34  65  66  91     99\n",
       "35  39  15  25     66\n",
       "36  49  54   7      8\n",
       "37  31  66   3     82\n",
       "38  20  27  51     17\n",
       "39  51   6  73     31\n",
       "40  40  18  21     90\n",
       "41   3   0  68     84\n",
       "42  33  94  24     21\n",
       "43  55  12  24     32\n",
       "44  78  97  99     52\n",
       "45  22  18  82      5\n",
       "46  59  60  67     74\n",
       "47  46  57  90     32\n",
       "48  29  56  56      4\n",
       "49  84  16  81     54"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's imagine df is our entire dataset and now we have to split the DataFrame into Training Set and Test Set.\n",
    "\n",
    "X = df[['f1','f2','f3']]     #Passing features / feature columns to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'] # 'label' we are trying to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok. So now we have features data set and label that we are trying to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Now_we_do_a_train_test_split_using_an_existing_train-test-split_library'></a>Now we do a train test split using an existing train-test-split library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)  \n",
    "# X is the 'What is the Input'\n",
    "# y is the 'What is the Output we need'\n",
    "# random_state is for 'repeatability' just like numpy's random set seed. In that way we can make sure we always get a same random split everytime you run the code\n",
    "# test_size is very situation specific number, most times we give 70% or 80% training size and 30% or 20% testing size. Sometimes 50% and 50% makes sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When we run the above code we get 4 variables X_train,X_test,y_train,y_test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape  #This is the 'feature' dataset for the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape  #This is the 'feature' dataset for the Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __So the basic idea here would be, once I actually have my neural network model working in pytorch / tensorflow and I want to do some sort of training process for supervised learning, I would feed it in the training sets for X_train and y_train and the model would try to basically build some sort of understanding of how the X training features are able to predict the y training labels__. \n",
    "* __Once they have that, then I can evaluate my Model by feeding it the 'X test data' and then it will try to predict what those labels should be. I can then compare those predictive values to the true 'y test values/data'. And that's the reason for a train-test-split (there by completes full evaluation of a dataset).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>[SciPy](#SciPy)\n",
    "<br>[Compute The Nth Derivate Of A Function](#Compute_The_Nth_Derivate_Of_A_Function)\n",
    "<br>[Permutation And Combinations](#Permutation_And_Combinations)\n",
    "<br>[Linear Algebra](#Linear_Algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='SciPy'></a>SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61BL8dcvicf_"
   },
   "source": [
    "* SciPy is a free and open-source Python library used for scientific computing and technical computing.\n",
    "* The SciPy library depends on NumPy, which provides convenient and fast N-dimensional array manipulation.\n",
    "* SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.\n",
    "* SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack\n",
    "\n",
    "***Learn more about SciPy here: https://docs.scipy.org/doc/***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNhLoX8vicgJ"
   },
   "outputs": [],
   "source": [
    "# import the scipy library\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2NS9J12icg0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the version of scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmHln7RwichH"
   },
   "source": [
    "# <a id='Compute_The_Nth_Derivate_Of_A_Function'></a>Compute The Nth Derivate Of A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAa-m1gCichM"
   },
   "outputs": [],
   "source": [
    "# import the derivative from scipy\n",
    "from scipy.misc import derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfs776jQichb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function\n",
    "def my_function(x):\n",
    "    return x**2 + x + 1\n",
    "\n",
    "# calculate the first derivative of the function at x = 2\n",
    "\n",
    "derivative(func= my_function, x0=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyvIZOGiichp"
   },
   "source": [
    "#### Function: `f(x) = x**2 + x + 1`\n",
    "#### Derivate:  `f'(x) = 2*x + 1`\n",
    "#### Solution:  `f'(2) = 2*2 + 1 = 5`\n",
    "\n",
    "---\n",
    "\n",
    "***Now, calculate the second derivative***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tvIPnd0icht"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative(func=my_function,x0=2,n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gy-Fxizfich4"
   },
   "source": [
    "# <a id='Permutation_And_Combinations'></a>Permutation And Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8zXWf9Vich8"
   },
   "outputs": [],
   "source": [
    "# COMBINATIONS\n",
    "from scipy.special import comb\n",
    "\n",
    "# total number of combinations from 4 different values taken 2 at a time\n",
    "# Value of 4C2\n",
    "com = comb(4, 2)\n",
    "\n",
    "print(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6JsIZMqiciL"
   },
   "outputs": [],
   "source": [
    "# PERMUTATIONS: Value of 4P2\n",
    "from scipy.special import perm\n",
    "per = perm(4, 2)\n",
    "\n",
    "print(per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_W1pIsAiciY"
   },
   "source": [
    "# <a id='Linear_Algebra'></a>Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VpDvrcdicib"
   },
   "outputs": [],
   "source": [
    "# import linear algebra module and numpy\n",
    "from scipy import linalg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNYJZAaTicim"
   },
   "outputs": [],
   "source": [
    "# square matrix\n",
    "matrix = np.array([[1, 5, 2],\n",
    "                   [3, 2, 1],\n",
    "                   [1, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSOBJvEIiciy"
   },
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "art0Ir25ici-"
   },
   "outputs": [],
   "source": [
    "# pass values to det() function\n",
    "linalg.det(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFXKWxjPicjK"
   },
   "outputs": [],
   "source": [
    "# inverse of a matrix\n",
    "linalg.inv(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
